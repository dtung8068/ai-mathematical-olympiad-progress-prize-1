{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "409cbbc9",
   "metadata": {
    "papermill": {
     "duration": 0.009374,
     "end_time": "2024-06-10T15:27:21.100196",
     "exception": false,
     "start_time": "2024-06-10T15:27:21.090822",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Forked from https://www.kaggle.com/code/abdurrafae/improved-code-interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f593c40",
   "metadata": {
    "papermill": {
     "duration": 0.008471,
     "end_time": "2024-06-10T15:27:21.117520",
     "exception": false,
     "start_time": "2024-06-10T15:27:21.109049",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Lewis:** the only changes in this notebook are those needed to run the original one with the new Kaggle evaluation API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1d4a434",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:27:21.136476Z",
     "iopub.status.busy": "2024-06-10T15:27:21.136119Z",
     "iopub.status.idle": "2024-06-10T15:27:27.950375Z",
     "shell.execute_reply": "2024-06-10T15:27:27.949664Z"
    },
    "papermill": {
     "duration": 6.826475,
     "end_time": "2024-06-10T15:27:27.952746",
     "exception": false,
     "start_time": "2024-06-10T15:27:21.126271",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24/2184763029.py:2: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pkg_resources\n",
    "import torch\n",
    "import time\n",
    "import aimo\n",
    "import gc\n",
    "import transformers\n",
    "import re\n",
    "import sys\n",
    "import subprocess\n",
    "import math\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from numpy.random import choice\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    AutoConfig,\n",
    "    StoppingCriteria,\n",
    "    set_seed,\n",
    "    StoppingCriteriaList\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9e53b64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:27:27.972829Z",
     "iopub.status.busy": "2024-06-10T15:27:27.972339Z",
     "iopub.status.idle": "2024-06-10T15:27:27.989468Z",
     "shell.execute_reply": "2024-06-10T15:27:27.988609Z"
    },
    "papermill": {
     "duration": 0.029491,
     "end_time": "2024-06-10T15:27:27.991496",
     "exception": false,
     "start_time": "2024-06-10T15:27:27.962005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "QUANT = False\n",
    "USE_PAST_KEY = True\n",
    "VALIDATION = False\n",
    "PRIVATE = True\n",
    "EXTERNAL_DF = '/kaggle/input/ai-mathematical-olympiad-prize/train.csv'\n",
    "\n",
    "if PRIVATE:\n",
    "    MODEL_PATH = \"/kaggle/input/deepseek-math\"#\"/kaggle/input/gemma/transformers/7b-it/1\"\n",
    "    DEEP = True\n",
    "\n",
    "#n_repetitions = 17 if PRIVATE else 4 # Original notebook had 22 but times out :(\n",
    "n_repetitions = 1\n",
    "TOTAL_TOKENS = 2048 # if PRIVATE else 512\n",
    "\n",
    "if PRIVATE:\n",
    "    TIME_LIMIT = 31500\n",
    "else:\n",
    "    TIME_LIMIT = 1\n",
    "    \n",
    "aops = \"\"\"Here are some example math problems along with explanations:\n",
    "\n",
    "Q1. Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?\n",
    "A1. This is a $d=st$ problem, so let $x$ be the time it takes to meet. We can write the following equation:\\[12x+18x=45\\]Solving gives us $x=1.5$. The $18x$ is Alicia so so $18\\times1.5=\\boxed{27}$\n",
    "\n",
    "Q2. How many positive perfect squares less than $2023$ are divisible by $5$?\n",
    "A2. Since $\\left \\lfloor{\\sqrt{2023}}\\right \\rfloor = 44$, there are $\\left \\lfloor{\\frac{44}{5}}\\right \\rfloor = \\boxed{8}$ perfect squares less than 2023.\n",
    "\n",
    "Q3. How many digits are in the base-ten representation of $8^5 \\cdot 5^{10} \\cdot 15^5$?\n",
    "A3. Prime factorizing this gives us $2^{15}\\cdot3^{5}\\cdot5^{15}=10^{15}\\cdot3^5=243\\cdot10^{15}$. $10^{15}$ has $16$ digits and $243$ = $2.43*10^{2}$ gives us $2$ more digits. $16+2=\\text{\\boxed{18}$. $2.43*10^{17}$ has $18$ digits\n",
    "\n",
    "Q4. A digital display shows the current date as an $8$-digit integer consisting of a $4$-digit year, followed by a $2$-digit month, followed by a $2$-digit date within the month. For example, Arbor Day this year is displayed as 20230428. For how many dates in $2023$ will each digit appear an even number of times in the 8-digital display for that date?\n",
    "A4. Do careful casework by each month. In the month and the date, we need a $0$, a $3$, and two digits repeated (which has to be $1$ and $2$ after consideration). After the casework, we get $\\boxed{9}$.\n",
    "\n",
    "Q5. Maureen is keeping track of the mean of her quiz scores this semester. If Maureen scores an $11$ on the next quiz, her mean will increase by $1$. If she scores an $11$ on each of the next three quizzes, her mean will increase by $2$. What is the mean of her quiz scores currently?\n",
    "A5. Let $a$ represent the amount of tests taken previously and $x$ the mean of the scores taken previously. We can write the following equations:\n",
    "\n",
    "\\[\\frac{ax+11}{a+1}=x+1\\qquad (1)\\]\\[\\frac{ax+33}{a+3}=x+2\\qquad (2)\\]\n",
    "\n",
    "Multiplying $(x+1)$ by $(a+1)$ and solving, we get:\\[ax+11=ax+a+x+1\\]\\[11=a+x+1\\]\\[a+x=10\\qquad (3)\\]\n",
    "\n",
    "Multiplying $(2)$ by $(a+3)$ and solving, we get:\\[ax+33=ax+2a+3x+6\\]\\[33=2a+3x+6\\]\\[2a+3x=27\\qquad (4)\\]\n",
    "\n",
    "Solving the system of equations for $(3)$ and $(4)$, we find that $a=3$ and $x=\\boxed{7}$.\n",
    "\n",
    "Q6. Positive real numbers $x$ and $y$ satisfy $y^3=x^2$ and $(y-x)^2=4y^2$. What is $x+y$?\n",
    "A6. Because $y^3=x^2$, set $x=a^3$, $y=a^2$ ($a\\neq 0$). Put them in $(y-x)^2=4y^2$ we get $(a^2(a-1))^2=4a^4$ which implies $a^2-2a+1=4$. Solve the equation to get $a=3$ or $-1$. Since $x$ and $y$ are positive, $a=3$ and $x+y=3^3+3^2=\\boxed{36}$.\n",
    "\n",
    "Q7. What is the degree measure of the acute angle formed by lines with slopes $2$ and $\\frac{1}{3}$?\n",
    "A7. Reminder that $\\text{slope}=\\dfrac{\\Delta y}{\\Delta x}=\\tan \\theta$ where $\\theta$ is the angle between the slope and $x$-axis. $k_1=2=\\tan \\alpha$, $k_2=\\dfrac{1}{3}=\\tan \\beta$. The angle formed by the two lines is $\\alpha-\\beta$. $\\tan(\\alpha-\\beta)=\\dfrac{\\tan\\alpha-\\tan\\beta}{1+\\tan\\alpha\\tan\\beta}=\\dfrac{2-1/3}{1+2\\cdot 1/3}=1$. Therefore, $\\alpha-\\beta=\\boxed{45}$.\n",
    "\n",
    "Q8. In a table tennis tournament every participant played every other participant exactly once. Although there were twice as many right-handed players as left-handed players, the number of games won by left-handed players was $40\\%$ more than the number of games won by right-handed players. (There were no ties and no ambidextrous players.) What is the total number of games played?\n",
    "A8. We know that the total amount of games must be the sum of games won by left and right handed players. Then, we can write $g = l + r$, and since $l = 1.4r$, $g = 2.4r$. Given that $r$ and $g$ are both integers, $g/2.4$ also must be an integer. From here we can see that $g$ must be divisible by 12, leaving only answers B and D. Now we know the formula for how many games are played in this tournament is $n(n-1)/2$, the sum of the first $n-1$ triangular numbers. Now, setting 36 and 48 equal to the equation will show that two consecutive numbers must have a product of 72 or 96. Clearly, $72=8*9$, so the answer is $\\boxed{36}$.\n",
    "\n",
    "Q9. How many complex numbers satisfy the equation $z^5=\\overline{z}$, where $\\overline{z}$ is the conjugate of the complex number $z$?\n",
    "A9. When $z^5=\\overline{z}$, there are two conditions: either $z=0$ or $z\\neq 0$. When $z\\neq 0$, since $|z^5|=|\\overline{z}|$, $|z|=1$. $z^5\\cdot z=z^6=\\overline{z}\\cdot z=|z|^2=1$. Consider the $r(\\cos \\theta +i\\sin \\theta)$ form, when $z^6=1$, there are 6 different solutions for $z$. Therefore, the number of complex numbers satisfying $z^5=\\bar{z}$ is $\\boxed{7}$.\n",
    "\n",
    "Q10. Consider the set of complex numbers $z$ satisfying $|1+z+z^{2}|=4$. The maximum value of the imaginary part of $z$ can be written in the form $\\tfrac{\\sqrt{m}}{n}$, where $m$ and $n$ are relatively prime positive integers. What is $m+n$?\n",
    "A10. First, substitute in $z=a+bi$.\n",
    "\n",
    "\\[|1+(a+bi)+(a+bi)^2|=4\\]\\[|(1+a+a^2-b^2)+ (b+2ab)i|=4\\]\\[(1+a+a^2-b^2)^2+ (b+2ab)^2=16\\]\\[(1+a+a^2-b^2)^2+ b^2(1+4a+4a^2)=16\\]\n",
    "Let $p=b^2$ and $q=1+a+a^2$\n",
    "\n",
    "\\[(q-p)^2+ p(4q-3)=16\\]\\[p^2-2pq+q^2   + 4pq -3p=16\\]\n",
    "We are trying to maximize $b=\\sqrt p$, so we'll turn the equation into a quadratic to solve for $p$ in terms of $q$.\n",
    "\n",
    "\\[p^2+(2q-3)p+(q^2-16)=0\\]\\[p=\\frac{(-2q+3)\\pm \\sqrt{-12q+73}}{2}\\]\n",
    "We want to maximize $p$. Since $q$ is always negatively contributing to $p$'s value, we want to minimize $q$.\n",
    "\n",
    "Due to the trivial inequality: $q=1+a+a^2=(a+\\frac 12)^2+\\frac{3}4 \\geq \\frac{3}4$\n",
    "\n",
    "If we plug $q$'s minimum value in, we get that $p$'s maximum value is\\[p=\\frac{(-2(\\frac 34)+3)+ \\sqrt{-12(\\frac 34)+73}}{2}=\\frac{\\frac 32+ 8}{2}=\\frac{19}{4}\\]\n",
    "Then\\[b=\\frac{\\sqrt{19}}{2}\\]and\\[m+n=\\boxed{21}]\n",
    "\n",
    "Q11. What is the product of all solutions to the equation\\[\\log_{7x}2023\\cdot \\log_{289x}2023=\\log_{2023x}2023\\]\n",
    "A11. For $\\log_{7x}2023\\cdot \\log_{289x}2023=\\log_{2023x}2023$, transform it into $\\dfrac{\\ln 289+\\ln 7}{\\ln 7 + \\ln x}\\cdot \\dfrac{\\ln 289+\\ln 7}{\\ln 289 + \\ln x}=\\dfrac{\\ln 289+\\ln 7}{\\ln 289+\\ln 7+\\ln x}$. Replace $\\ln x$ with $y$. Because we want to find the product of all solutions of $x$, it is equivalent to finding the exponential of the sum of all solutions of $y$. Change the equation to standard quadratic equation form, the term with 1 power of $y$ is canceled. By using Vieta, we see that since there does not exist a $by$ term, $\\sum y=0$ and $\\prod x=e^0=\\boxed{1}$.\n",
    "\n",
    "Q12. Rows 1, 2, 3, 4, and 5 of a triangular array of integers are shown below.\n",
    "\n",
    "[asy] size(4.5cm); label(\"$1$\", (0,0)); label(\"$1$\", (-0.5,-2/3)); label(\"$1$\", (0.5,-2/3)); label(\"$1$\", (-1,-4/3)); label(\"$3$\", (0,-4/3)); label(\"$1$\", (1,-4/3)); label(\"$1$\", (-1.5,-2)); label(\"$5$\", (-0.5,-2)); label(\"$5$\", (0.5,-2)); label(\"$1$\", (1.5,-2)); label(\"$1$\", (-2,-8/3)); label(\"$7$\", (-1,-8/3)); label(\"$11$\", (0,-8/3)); label(\"$7$\", (1,-8/3)); label(\"$1$\", (2,-8/3)); [/asy]\n",
    "Each row after the first row is formed by placing a 1 at each end of the row, and each interior entry is 1 greater than the sum of the two numbers diagonally above it in the previous row. What is the units digits of the sum of the 2023 numbers in the 2023rd row?\n",
    "A12. First, let $R(n)$ be the sum of the $n$th row. Now, with some observation and math instinct, we can guess that $R(n) = 2^n - n$.\n",
    "\n",
    "Now we try to prove it by induction,\n",
    "\n",
    "$R(1) = 2^n - n = 2^1 - 1 = 1$ (works for base case)\n",
    "\n",
    "$R(k) = 2^k - k$\n",
    "\n",
    "$R(k+1) = 2^{k+1} - (k + 1) = 2(2^k) - k - 1$\n",
    "\n",
    "By definition from the question, the next row is always$:$\n",
    "\n",
    "Double the sum of last row (Imagine each number from last row branches off toward left and right to the next row), plus # of new row, minus 2 (minus leftmost and rightmost's 1)\n",
    "\n",
    "Which gives us $:$\n",
    "\n",
    "$2(2^k - k) + (k + 1) - 2 = 2(2^k) - k - 1$\n",
    "\n",
    "Hence, proven\n",
    "\n",
    "Last, simply substitute $n = 2023$, we get $R(2023) = 2^{2023} - 2023$\n",
    "\n",
    "Last digit of $2^{2023}$ is $8$, $8-3 = \\boxed{5}$\n",
    "\n",
    "Q13. Let $f$ be the unique function defined on the positive integers such that\\[\\sum_{d\\mid n}d\\cdot f\\left(\\frac{n}{d}\\right)=1\\]for all positive integers $n$. What is $f(2023)$?\n",
    "A13. From the problem, we want to find $f(2023)$. Using the problem, we get $f(2023)+7f(289)+17f(119)+119f(17)+289f(7)+2023f(1)=1$. By plugging in factors of $2023$, we get\\begin{align} f(7)+7f(1)=1\\\\ f(17)+17f(1)=1\\\\ f(119)+7f(17)+17f(7)+119f(1)=1\\\\ f(289)+17f(17)+289f(1)=1 \\end{align}Notice that $(4)-17(2)=f(289)$, so $f(289)=-16$. Similarly, notice that $(3)-17(1)=f(119)+7f(17)=-16$. Now, substituting this all back into our equation to solve for $f(2023)$, we get\\begin{align*} f(2023)+7f(289)+17(f(119)+7f(17))+289(f(7)+7f(1))=1\\\\ f(2023)+7 \\cdot (-16) + 17 \\cdot (-16) + 289 \\cdot (1) = 1\\\\ f(2023)=\\boxed{96} \\end{align*}\n",
    "\n",
    "Q14. How many ordered pairs of positive real numbers $(a,b)$ satisfy the equation\\[(1+2a)(2+2b)(2a+b) = 32ab?\\]\n",
    "A14. Using AM-GM on the two terms in each factor on the left, we get\\[(1+2a)(2+2b)(2a+b) \\ge 8\\sqrt{2a \\cdot 4b \\cdot 2ab}= 32ab,\\]meaning the equality condition must be satisfied. This means $1 = 2a = b$, so we only have $\\boxed{1}$ solution.\n",
    "\n",
    "Q15. Let $K$ be the number of sequences $A_1$, $A_2$, $\\dots$, $A_n$ such that $n$ is a positive integer less than or equal to $10$, each $A_i$ is a subset of $\\{1, 2, 3, \\dots, 10\\}$, and $A_{i-1}$ is a subset of $A_i$ for each $i$ between $2$ and $n$, inclusive. For example, $\\{\\}$, $\\{5, 7\\}$, $\\{2, 5, 7\\}$, $\\{2, 5, 7\\}$, $\\{2, 5, 6, 7, 9\\}$ is one such sequence, with $n = 5$.What is the remainder when $K$ is divided by $10$?\n",
    "A15. Consider any sequence with $n$ terms. Every 10 number has such choices: never appear, appear the first time in the first spot, appear the first time in the second spot… and appear the first time in the $n$th spot, which means every number has $(n+1)$ choices to show up in the sequence. Consequently, for each sequence with length $n$, there are $(n+1)^{10}$ possible ways.\n",
    "\n",
    "Thus, the desired value is $\\sum_{i=1}^{10}(i+1)^{10}\\equiv \\boxed{5}\\pmod{10}$\n",
    "\n",
    "\"\"\"    \n",
    "    \n",
    "code = aops + \"\"\"Below is another math problem you are to solve (non-negative integer):\n",
    "\\\"{}\\\"\n",
    "Determine a sympy-based approach for solving the problem by listing each step to take and what functions need to be called in each step. Remember, your final answer should be a non-negative integer, not an algebraic expression.\n",
    "Write the entire script covering all the steps (use comments and document it well) and print the result. After solving the problem, output the final numerical answer within \\\\boxed{}.\n",
    "\n",
    "Approach:\"\"\"\n",
    "\n",
    "\n",
    "cot = \"\"\"Below is a math problem you are to solve (non-negative integer):\n",
    "\\\"{}\\\"\n",
    "Analyze this problem and think step by step to come to a solution with programs. After solving the problem, output the final numerical answer within \\\\boxed{}.\\n\\n\"\"\"\n",
    "\n",
    "promplt_options = [code,cot]\n",
    "    \n",
    "tool_instruction = '\\n\\nPlease integrate natural language reasoning with programs to solve the above problem, and put your final numerical answer within \\\\boxed{}.\\nNote that the intermediary calculations may be real numbers, but the final numerical answer would always be an integer.'\n",
    "\n",
    "#tool_instruction = \" The answer should be given as a non-negative modulo 1000.\"\n",
    "#tool_instruction += '\\nPlease integrate natural language reasoning with programs to solve the problem above, and put your final answer within \\\\boxed{}.'\n",
    "\n",
    "temperature = 0.9\n",
    "top_p = 1.0\n",
    "\n",
    "temperature_coding = 0.9\n",
    "top_p_coding = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a193fb95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:27:28.011019Z",
     "iopub.status.busy": "2024-06-10T15:27:28.010296Z",
     "iopub.status.idle": "2024-06-10T15:27:28.014872Z",
     "shell.execute_reply": "2024-06-10T15:27:28.014074Z"
    },
    "papermill": {
     "duration": 0.016186,
     "end_time": "2024-06-10T15:27:28.016623",
     "exception": false,
     "start_time": "2024-06-10T15:27:28.000437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_package_version(package_name):\n",
    "    try:\n",
    "        version = pkg_resources.get_distribution(package_name).version\n",
    "        return version\n",
    "    except pkg_resources.DistributionNotFound:\n",
    "        return \"Package not found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c17ff354",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:27:28.036021Z",
     "iopub.status.busy": "2024-06-10T15:27:28.035322Z",
     "iopub.status.idle": "2024-06-10T15:27:28.042114Z",
     "shell.execute_reply": "2024-06-10T15:27:28.041285Z"
    },
    "papermill": {
     "duration": 0.018729,
     "end_time": "2024-06-10T15:27:28.044219",
     "exception": false,
     "start_time": "2024-06-10T15:27:28.025490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python版本： 3.10.13 | packaged by conda-forge | (main, Dec 23 2023, 15:36:39) [GCC 12.3.0]\n",
      "torch版本：2.1.2\n",
      "CUDA版本： 12.1\n"
     ]
    }
   ],
   "source": [
    "print('python版本：',sys.version)\n",
    "\n",
    "package_name = \"torch\"\n",
    "version = get_package_version(package_name)\n",
    "print(f\"{package_name}版本：{version}\")\n",
    "\n",
    "cuda_version = torch.version.cuda\n",
    "print(\"CUDA版本：\", cuda_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9367495",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:27:28.062791Z",
     "iopub.status.busy": "2024-06-10T15:27:28.062518Z",
     "iopub.status.idle": "2024-06-10T15:27:28.066170Z",
     "shell.execute_reply": "2024-06-10T15:27:28.065390Z"
    },
    "papermill": {
     "duration": 0.014985,
     "end_time": "2024-06-10T15:27:28.067995",
     "exception": false,
     "start_time": "2024-06-10T15:27:28.053010",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Forked From  https://kaggle.com/code/xiaoz259/pure-rng/notebook\n",
    "\n",
    "\n",
    "# credits:\n",
    "# https://www.kaggle.com/code/olyatsimboy/aimo-openmath-mistral-baseline\n",
    "# https://www.kaggle.com/code/aatiffraz/prompt-prediction-w-mixtral-mistral7b-gemma-llama\n",
    "# https://www.kaggle.com/code/thedrcat/aimo-mixtral-baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b332f8df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:27:28.086658Z",
     "iopub.status.busy": "2024-06-10T15:27:28.086389Z",
     "iopub.status.idle": "2024-06-10T15:27:28.090167Z",
     "shell.execute_reply": "2024-06-10T15:27:28.089390Z"
    },
    "papermill": {
     "duration": 0.015253,
     "end_time": "2024-06-10T15:27:28.092113",
     "exception": false,
     "start_time": "2024-06-10T15:27:28.076860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NOTEBOOK_START_TIME = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a65bcd05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:27:28.111842Z",
     "iopub.status.busy": "2024-06-10T15:27:28.111349Z",
     "iopub.status.idle": "2024-06-10T15:27:28.115317Z",
     "shell.execute_reply": "2024-06-10T15:27:28.114561Z"
    },
    "papermill": {
     "duration": 0.016235,
     "end_time": "2024-06-10T15:27:28.117125",
     "exception": false,
     "start_time": "2024-06-10T15:27:28.100890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = aimo.make_env()\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53e2b8f",
   "metadata": {
    "papermill": {
     "duration": 0.008682,
     "end_time": "2024-06-10T15:27:28.134650",
     "exception": false,
     "start_time": "2024-06-10T15:27:28.125968",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "TO-DO\n",
    "\n",
    "Change temperature as the question goes longer\n",
    "Change temperature based on question lenght"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8021b213",
   "metadata": {
    "papermill": {
     "duration": 0.008481,
     "end_time": "2024-06-10T15:27:28.152198",
     "exception": false,
     "start_time": "2024-06-10T15:27:28.143717",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Zero-shot MMOS-DeepSeekMath-7B with self-consistency and generated code reasoning evaluation\n",
    "\n",
    "Self-consistency is a modification of the standard greedy decoding in reasoning pipelines via sampling several diverse answers followed by aggregation, e.g., most common answer ([SC-CoT paper](https://arxiv.org/pdf/2203.11171.pdf)).\n",
    "\n",
    "In this kernel, we will consider MMOS-DeepSeekMath-7B RL-tuned backbone; in my experiments, this model produces more consistent code reasoning and the code block execution will allow us to decrease arithmetic hallucinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c937984",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:27:28.171764Z",
     "iopub.status.busy": "2024-06-10T15:27:28.171013Z",
     "iopub.status.idle": "2024-06-10T15:27:28.175613Z",
     "shell.execute_reply": "2024-06-10T15:27:28.174747Z"
    },
    "papermill": {
     "duration": 0.016586,
     "end_time": "2024-06-10T15:27:28.177651",
     "exception": false,
     "start_time": "2024-06-10T15:27:28.161065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if QUANT:\n",
    "    from transformers import BitsAndBytesConfig\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8a89cfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:27:28.196815Z",
     "iopub.status.busy": "2024-06-10T15:27:28.196120Z",
     "iopub.status.idle": "2024-06-10T15:27:38.781185Z",
     "shell.execute_reply": "2024-06-10T15:27:38.780072Z"
    },
    "papermill": {
     "duration": 10.596966,
     "end_time": "2024-06-10T15:27:38.783443",
     "exception": false,
     "start_time": "2024-06-10T15:27:28.186477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers Version: 4.39.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 15:27:30.017283: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-10 15:27:30.017404: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-10 15:27:30.146255: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.2 s, sys: 1.04 s, total: 7.24 s\n",
      "Wall time: 10.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if QUANT:\n",
    "    !pip install -U /kaggle/input/accelerate-wheelwhl/accelerate-0.29.1-py3-none-any.whl -qq\n",
    "    !pip install -U /kaggle/input/bitsandbytes-0-42-0-py3-none-any-whl/bitsandbytes-0.42.0-py3-none-any.whl -qq\n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "\n",
    "print(f\"Transformers Version: {transformers.__version__}\")\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9608de42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:27:38.804112Z",
     "iopub.status.busy": "2024-06-10T15:27:38.803584Z",
     "iopub.status.idle": "2024-06-10T15:27:38.807660Z",
     "shell.execute_reply": "2024-06-10T15:27:38.806822Z"
    },
    "papermill": {
     "duration": 0.015939,
     "end_time": "2024-06-10T15:27:38.809437",
     "exception": false,
     "start_time": "2024-06-10T15:27:38.793498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-prize/test.csv')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4af45c90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:27:38.829179Z",
     "iopub.status.busy": "2024-06-10T15:27:38.828903Z",
     "iopub.status.idle": "2024-06-10T15:27:38.832556Z",
     "shell.execute_reply": "2024-06-10T15:27:38.831725Z"
    },
    "papermill": {
     "duration": 0.01567,
     "end_time": "2024-06-10T15:27:38.834405",
     "exception": false,
     "start_time": "2024-06-10T15:27:38.818735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if len(df) < 5:\n",
    "#     df = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-prize/train.csv')\n",
    "#     PRIVATE = False\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf515b42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:27:38.853918Z",
     "iopub.status.busy": "2024-06-10T15:27:38.853464Z",
     "iopub.status.idle": "2024-06-10T15:27:38.859453Z",
     "shell.execute_reply": "2024-06-10T15:27:38.858603Z"
    },
    "papermill": {
     "duration": 0.017596,
     "end_time": "2024-06-10T15:27:38.861227",
     "exception": false,
     "start_time": "2024-06-10T15:27:38.843631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def naive_parse(answer):\n",
    "    out = []\n",
    "    start = False\n",
    "    end = False\n",
    "    for l in reversed(list(answer)):\n",
    "        if l in '0123456789' and not end:\n",
    "            start = True\n",
    "            out.append(l)\n",
    "        else:\n",
    "            if start:\n",
    "                end = True\n",
    "        \n",
    "    out = reversed(out)\n",
    "    return ''.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2dd68ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:27:38.880908Z",
     "iopub.status.busy": "2024-06-10T15:27:38.880394Z",
     "iopub.status.idle": "2024-06-10T15:27:38.894190Z",
     "shell.execute_reply": "2024-06-10T15:27:38.893368Z"
    },
    "papermill": {
     "duration": 0.025451,
     "end_time": "2024-06-10T15:27:38.896005",
     "exception": false,
     "start_time": "2024-06-10T15:27:38.870554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def return_last_print(output, n):\n",
    "    lines = output.strip().split('\\n')\n",
    "    if lines:\n",
    "        return lines[n]\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def process_code(code, return_shell_output=False):\n",
    "    \n",
    "    def repl(match):\n",
    "        if \"real\" not in match.group():\n",
    "            return \"{}{}\".format(match.group()[:-1], ', real=True)')\n",
    "        else:\n",
    "            return \"{}{}\".format(match.group()[:-1], ')')\n",
    "    code = re.sub(r\"symbols\\([^)]+\\)\", repl, code)\n",
    "\n",
    "    if return_shell_output:\n",
    "        code = code.replace('\\n', '\\n    ')\n",
    "            # Add a try...except block\n",
    "        code = \"\\ntry:\\n    from sympy import *\\n{}\\nexcept Exception as e:\\n    print(e)\\n    print('FAIL')\\n\".format(code)\n",
    "    \n",
    "    if not return_shell_output:\n",
    "        print(code)\n",
    "    with open('code.py', 'w') as fout:\n",
    "        fout.write(code)\n",
    "    \n",
    "    batcmd = 'timeout 7 ' + sys.executable + ' code.py'\n",
    "    try:\n",
    "        shell_output = subprocess.check_output(batcmd, shell=True).decode('utf8')\n",
    "        return_value = return_last_print(shell_output, -1)\n",
    "        print(shell_output)\n",
    "        if return_shell_output:\n",
    "            if return_value=='FAIL':\n",
    "                CODE_STATUS = False\n",
    "                return_value = return_last_print(shell_output, -2)\n",
    "                if \"not defined\" in return_value:\n",
    "                    return_value+='\\nTry checking the formatting and imports'\n",
    "            else:\n",
    "                CODE_STATUS = True\n",
    "            return return_value, CODE_STATUS  \n",
    "        code_output = round(float(eval(return_value))) % 1000\n",
    "    except Exception as e:\n",
    "        print(e,'shell_output')\n",
    "        code_output = -1\n",
    "    \n",
    "    if return_shell_output:\n",
    "        if code_output==-1:\n",
    "            CODE_STATUS = False\n",
    "        else:\n",
    "            CODE_STATUS = True\n",
    "        return code_output, CODE_STATUS  \n",
    "    \n",
    "    \n",
    "    return code_output\n",
    "\n",
    "\n",
    "def process_text_output(output):\n",
    "    result = output    \n",
    "    try:\n",
    "        result_output = re.findall(r'\\\\boxed\\{(\\d+)\\}', result)\n",
    "\n",
    "        print('BOXED', result_output)\n",
    "        if not len(result_output):\n",
    "            result_output = naive_parse(result)\n",
    "        else:\n",
    "            result_output = result_output[-1]\n",
    "\n",
    "        print('BOXED FINAL', result_output)\n",
    "        if not len(result_output):\n",
    "            result_output = -1\n",
    "        \n",
    "        else:\n",
    "            result_output = round(float(eval(result_output))) % 1000\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('ERROR PARSING TEXT')\n",
    "        result_output = -1\n",
    "    \n",
    "    return result_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "902846e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:27:38.915103Z",
     "iopub.status.busy": "2024-06-10T15:27:38.914839Z",
     "iopub.status.idle": "2024-06-10T15:27:39.141940Z",
     "shell.execute_reply": "2024-06-10T15:27:39.141036Z"
    },
    "papermill": {
     "duration": 0.239163,
     "end_time": "2024-06-10T15:27:39.144102",
     "exception": false,
     "start_time": "2024-06-10T15:27:38.904939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62ff8d74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:27:39.165039Z",
     "iopub.status.busy": "2024-06-10T15:27:39.164685Z",
     "iopub.status.idle": "2024-06-10T15:30:06.974885Z",
     "shell.execute_reply": "2024-06-10T15:30:06.973875Z"
    },
    "papermill": {
     "duration": 147.82341,
     "end_time": "2024-06-10T15:30:06.977226",
     "exception": false,
     "start_time": "2024-06-10T15:27:39.153816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cd830727ea24211ac46cce854c3c34c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if PRIVATE:\n",
    "    config = AutoConfig.from_pretrained(MODEL_PATH)\n",
    "    config.gradient_checkpointing = True\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "    device_map = [('model.embed_tokens', 0),\n",
    "                 ('model.layers.0', 0),\n",
    "                 ('model.layers.1', 0),\n",
    "                 ('model.layers.2', 0),\n",
    "                 ('model.layers.3', 0),\n",
    "                 ('model.layers.4', 0),\n",
    "                 ('model.layers.5', 0),\n",
    "                 ('model.layers.6', 0),\n",
    "                 ('model.layers.7', 0),\n",
    "                 ('model.layers.8', 0),\n",
    "                 ('model.layers.9', 0),\n",
    "                 ('model.layers.10', 0),\n",
    "                 ('model.layers.11', 0),\n",
    "                 ('model.layers.12', 0),\n",
    "                 ('model.layers.13', 0),\n",
    "                 ('model.layers.14', 0),\n",
    "                 ('model.layers.15', 0),\n",
    "                 ('model.layers.16', 0),\n",
    "                 ('model.layers.17', 0),\n",
    "                 ('model.layers.18', 0),\n",
    "                 ('model.layers.19', 0),\n",
    "                 ('model.layers.20', 0),\n",
    "                 ('model.layers.21', 0),\n",
    "                 ('model.layers.22', 1),\n",
    "                 ('model.layers.23', 1),\n",
    "                 ('model.layers.24', 1),\n",
    "                 ('model.layers.25', 1),\n",
    "                 ('model.layers.26', 1),\n",
    "                 ('model.layers.27', 1),\n",
    "                 ('model.layers.28', 1),\n",
    "                 ('model.layers.29', 1),\n",
    "                 ('model.norm', 1),\n",
    "                 ('lm_head', 1)]\n",
    "\n",
    "    device_map = {ii:jj for (ii,jj) in device_map}\n",
    "\n",
    "    if QUANT:\n",
    "        from transformers import BitsAndBytesConfig\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit = True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_PATH,\n",
    "            device_map=\"sequential\",\n",
    "            torch_dtype=\"auto\",\n",
    "            trust_remote_code=True, \n",
    "            quantization_config=quantization_config,\n",
    "            config=config\n",
    "        )\n",
    "    else:  \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_PATH,\n",
    "            device_map=device_map,\n",
    "            torch_dtype=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            #quantization_config=quantization_config,\n",
    "            config=config\n",
    "        )\n",
    "    \n",
    "    pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype='auto',\n",
    "    device_map=device_map,\n",
    ")\n",
    "\n",
    "    class StoppingCriteriaSub(StoppingCriteria):\n",
    "        def __init__(self, stops = [], encounters=1):\n",
    "            super().__init__()\n",
    "            self.stops = [stop.to(\"cuda\") for stop in stops]\n",
    "\n",
    "        def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "            for stop in self.stops:\n",
    "                last_token = input_ids[0][-len(stop):]\n",
    "                if torch.all(torch.eq(stop,last_token)):\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "\n",
    "    stop_words = [\"```output\", \"```python\", \"```\\nOutput\" , \")\\n```\" , \"``````output\"] #,  \n",
    "    stop_words_ids = [tokenizer(stop_word, return_tensors='pt', add_special_tokens=False)['input_ids'].squeeze() for stop_word in stop_words]\n",
    "    stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])\n",
    "    \n",
    "    model.dtype, model.hf_device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff15e81f",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-06-10T15:30:06.998034Z",
     "iopub.status.busy": "2024-06-10T15:30:06.997739Z",
     "iopub.status.idle": "2024-06-10T15:30:29.684536Z",
     "shell.execute_reply": "2024-06-10T15:30:29.683505Z"
    },
    "papermill": {
     "duration": 22.699629,
     "end_time": "2024-06-10T15:30:29.686615",
     "exception": false,
     "start_time": "2024-06-10T15:30:06.986986",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n",
      "Solving problem 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "QUESTION 0 - 0 - TIME_SPENT : 159 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.23s/it]\n",
      "1it [00:02,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacement index 27 out of range for positional args tuple 5\n",
      "code_answers 0 text_answers 0\n",
      "Predicted best answer: {0: (-1, -1)}\n",
      "Solving problem 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "QUESTION 1 - 0 - TIME_SPENT : 161 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_User: Below is a math problem you are to solve (non-negative integer):\n",
      "\"What is $0\\times10$?\"\n",
      "Analyze this problem and think step by step to come to a solution with programs. After solving the problem, output the final numerical answer within \\boxed{}.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "\n",
      "\n",
      "INTERMEDIATE OUT :\n",
      "\n",
      "def multiplication():\n",
      "    \"\"\"What is $0\\times10$?\"\"\"\n",
      "    return 0 * 10\n",
      "\n",
      "result = multiplication()\n",
      "print(result)\n",
      "```\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      "CODE RESULTS 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:10<00:00, 10.02s/it]\n",
      "2it [00:12,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INTERMEDIATE OUT :\n",
      "```output\n",
      "0\n",
      "```\n",
      "The value of $0\\times10$ is 0. The answer is $\\boxed{0}$.\n",
      "\n",
      "BOXED ['0']\n",
      "BOXED FINAL 0\n",
      "[(0, 2)]\n",
      "GOOD ANSWER UPDATED!\n",
      "code_answers 1 text_answers 1\n",
      "Predicted best answer: {0: (-1, -1), 1: (0, 2)}\n",
      "Solving problem 2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "QUESTION 2 - 0 - TIME_SPENT : 171 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_User: Below is a math problem you are to solve (non-negative integer):\n",
      "\"Solve $4+x=4$ for $x$.\"\n",
      "Analyze this problem and think step by step to come to a solution with programs. After solving the problem, output the final numerical answer within \\boxed{}.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "\n",
      "\n",
      "INTERMEDIATE OUT :\n",
      "\n",
      "from sympy import symbols, Eq, solve\n",
      "\n",
      "def solve_equation():\n",
      "    \"\"\"Solve $4+x=4$ for $x$.\"\"\"\n",
      "    x = symbols('x')\n",
      "    equation = Eq(4 + x, 4)\n",
      "    solution = solve(equation, x)[0]\n",
      "\n",
      "    return solution\n",
      "\n",
      "result = solve_equation()\n",
      "print(result)\n",
      "```\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      "CODE RESULTS 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:10<00:00, 10.35s/it]\n",
      "3it [00:22,  7.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INTERMEDIATE OUT :\n",
      "```output\n",
      "0\n",
      "```\n",
      "The solution for $x$ is $0$.\n",
      "The answer is $\\boxed{0}$.\n",
      "\n",
      "BOXED ['0']\n",
      "BOXED FINAL 0\n",
      "[(0, 2)]\n",
      "GOOD ANSWER UPDATED!\n",
      "code_answers 1 text_answers 1\n",
      "Predicted best answer: {0: (-1, -1), 1: (0, 2), 2: (0, 2)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_results = {}\n",
    "total_answers = {}\n",
    "best_stats = {}\n",
    "total_outputs = {}\n",
    "question_type_counts = {}\n",
    "starting_counts = (2,3)\n",
    "\n",
    "if VALIDATION:\n",
    "    x = pd.read_csv(EXTERNAL_DF)\n",
    "    y = x.copy()\n",
    "    y['row_id'] = range(0, len(x))\n",
    "    y['answer'] = 0\n",
    "    y = y.drop(columns=['problem'])\n",
    "    iterable = zip(x.iterrows(), y.iterrows())\n",
    "else:\n",
    "    iterable = iter_test\n",
    "    \n",
    "# LEWIS: I had to invert the loop order because the new API forbids repeated calls on the same problem\n",
    "for i, (test, sample_submission) in tqdm(enumerate(iterable)):\n",
    "    print(f\"Solving problem {i} ...\")\n",
    "    TIME_SPENT = time.time() - NOTEBOOK_START_TIME\n",
    "\n",
    "    if TIME_SPENT>TIME_LIMIT:\n",
    "        sample_submission['answer'] = 0\n",
    "        env.predict(sample_submission)\n",
    "        break\n",
    "        \n",
    "    for jj in tqdm(range(n_repetitions)):   \n",
    "#         id_ = df['id'].loc[i]\n",
    "#         problem = df['problem'].loc[i]\n",
    "\n",
    "        if VALIDATION:\n",
    "            problem = test[1]['problem']\n",
    "        else:\n",
    "            problem = test['problem'].values[0]\n",
    "        print(f\"\\n\\n\\nQUESTION {i} - {jj} - TIME_SPENT : {TIME_SPENT:.0f} secs\")\n",
    "        \n",
    "        best, best_count = best_stats.get(i,(-1,-1))\n",
    "        if best_count>np.sqrt(jj):\n",
    "            print(\"SKIPPING CAUSE ALREADY FOUND BEST\")\n",
    "            continue\n",
    "            \n",
    "        outputs = total_outputs.get(i,[])\n",
    "        text_answers, code_answers = question_type_counts.get(i,starting_counts)\n",
    "        results = total_results.get(i,[])\n",
    "        answers = total_answers.get(i,[])\n",
    "        \n",
    "        for _ in range(5):\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            time.sleep(0.2)\n",
    "\n",
    "        try:\n",
    "            ALREADY_GEN = 0\n",
    "            code_error = None\n",
    "            code_error_count = 0\n",
    "            code_output = -1\n",
    "            #initail_message = problem  + tool_instruction \n",
    "            counts = np.array([text_answers,code_answers])\n",
    "\n",
    "            draw = choice(promplt_options, 1,\n",
    "                          p=counts/counts.sum())\n",
    "\n",
    "            initail_message = draw[0].format(problem,\"{}\")            \n",
    "            prompt = f\"User: {initail_message}\"\n",
    "\n",
    "            current_printed = len(prompt)\n",
    "            print(f\"{jj}_{prompt}\\n\")\n",
    "\n",
    "            model_inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "            input_len = len(model_inputs['input_ids'][0])\n",
    "\n",
    "            generation_output = model.generate(**model_inputs, \n",
    "                                               max_new_tokens=TOTAL_TOKENS-ALREADY_GEN,\n",
    "                                               return_dict_in_generate=USE_PAST_KEY,\n",
    "                                               do_sample = True,\n",
    "                                               temperature = temperature,\n",
    "                                               top_p = top_p,\n",
    "                                               num_return_sequences=1, stopping_criteria = stopping_criteria)\n",
    "\n",
    "            if USE_PAST_KEY:\n",
    "                output_ids = generation_output.sequences[0]\n",
    "            else:\n",
    "                output_ids = generation_output[0]\n",
    "            decoded_output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "            print(f\"{decoded_output[current_printed:]}\\n\")\n",
    "            current_printed += len(decoded_output[current_printed:])\n",
    "            cummulative_code = \"\"\n",
    "            \n",
    "            \n",
    "            stop_word_cond = False\n",
    "            for stop_word in stop_words:\n",
    "                stop_word_cond = stop_word_cond or (decoded_output[-len(stop_word):]==stop_word)\n",
    "                \n",
    "            \n",
    "            while (stop_word_cond) and (ALREADY_GEN<(TOTAL_TOKENS)):\n",
    "\n",
    "                if (decoded_output[-len(\"```python\"):]==\"```python\"):\n",
    "                    temperature_inner=temperature_coding\n",
    "                    top_p_inner = top_p_coding\n",
    "                    prompt = decoded_output\n",
    "                else:\n",
    "                    temperature_inner=temperature\n",
    "                    top_p_inner = top_p\n",
    "                    try:\n",
    "                        if (decoded_output[-len(\"``````output\"):]==\"``````output\"):\n",
    "                            code_text = decoded_output.split('```python')[-1].split(\"``````\")[0]\n",
    "                        else:\n",
    "                            code_text = decoded_output.split('```python')[-1].split(\"```\")[0]\n",
    "                        \n",
    "\n",
    "                        cummulative_code+=code_text\n",
    "                        code_output, CODE_STATUS = process_code(cummulative_code, return_shell_output=True)\n",
    "                        print('CODE RESULTS', code_output)\n",
    "\n",
    "                        if code_error==code_output:\n",
    "                            code_error_count+=1\n",
    "                        else:\n",
    "                            code_error=code_output\n",
    "                            code_error_count = 0\n",
    "\n",
    "                        if not CODE_STATUS:\n",
    "                            cummulative_code = cummulative_code[:-len(code_text)]\n",
    "\n",
    "                            if code_error_count>=1:\n",
    "                                print(\"REPEATED ERRORS\")\n",
    "                                break\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        print('ERROR PARSING CODE')\n",
    "                        code_output = -1\n",
    "\n",
    "                    if code_output!=-1:\n",
    "                        if (decoded_output[-len(\")\\n```\"):]==\")\\n```\"):\n",
    "                            prompt = decoded_output+'```output\\n'+str(code_output)+'\\n```\\n'\n",
    "                        else:\n",
    "                            prompt = decoded_output+'\\n'+str(code_output)+'\\n```\\n'\n",
    "                    else:\n",
    "                        prompt = decoded_output\n",
    "                        cummulative_code=\"\"\n",
    "\n",
    "\n",
    "                model_inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "                ALREADY_GEN =  len(model_inputs['input_ids'][0])-input_len\n",
    "\n",
    "                if USE_PAST_KEY:\n",
    "                    old_values = generation_output.past_key_values\n",
    "                else:\n",
    "                    old_values = None\n",
    "\n",
    "                generation_output = model.generate(**model_inputs, \n",
    "                                                   max_new_tokens=TOTAL_TOKENS-ALREADY_GEN, \n",
    "                                                   return_dict_in_generate=USE_PAST_KEY,\n",
    "                                                   past_key_values=old_values,\n",
    "                                                   do_sample = True,\n",
    "                                                   temperature = temperature_inner,\n",
    "                                                   top_p = top_p_inner,\n",
    "                                                   num_return_sequences=1, stopping_criteria = stopping_criteria)\n",
    "\n",
    "                if USE_PAST_KEY:\n",
    "                    output_ids = generation_output.sequences[0]\n",
    "                else:\n",
    "                    output_ids = generation_output[0]\n",
    "                decoded_output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "                print(f\"\\nINTERMEDIATE OUT :\\n{decoded_output[current_printed:]}\\n\")\n",
    "                current_printed+=len(decoded_output[current_printed:])\n",
    "                \n",
    "                stop_word_cond = False\n",
    "                for stop_word in stop_words:\n",
    "                    stop_word_cond = stop_word_cond or (decoded_output[-len(stop_word):]==stop_word)\n",
    "\n",
    "            if USE_PAST_KEY:\n",
    "                output_ids = generation_output.sequences[0]\n",
    "            else:\n",
    "                output_ids = generation_output[0]\n",
    "\n",
    "            raw_output = tokenizer.decode(output_ids[input_len:], skip_special_tokens=True)\n",
    "            #print(f\"\\n\\nOutput :\\n{raw_output}\\n\")                            \n",
    "            result_output = process_text_output(raw_output)\n",
    "            \n",
    "            try:\n",
    "                code_output = round(float(eval(code_output))) % 1000\n",
    "            except Exception as e:\n",
    "                print(e,'final_eval')\n",
    "                code_output = -1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e,\"5\")\n",
    "            result_output, code_output = -1, -1\n",
    "\n",
    "        if code_output!=-1:\n",
    "            outputs.append(code_output)\n",
    "            code_answers+=1\n",
    "\n",
    "        if result_output!=-1:\n",
    "            outputs.append(result_output)\n",
    "            text_answers+=1\n",
    "\n",
    "        if len(outputs) > 0:\n",
    "            occurances = Counter(outputs).most_common()\n",
    "            print(occurances)\n",
    "            if occurances[0][1] > best_count:\n",
    "                print(\"GOOD ANSWER UPDATED!\")\n",
    "                best = occurances[0][0]\n",
    "                best_count = occurances[0][1]\n",
    "            if occurances[0][1] > 5:\n",
    "                print(\"ANSWER FOUND!\")\n",
    "                break\n",
    "\n",
    "        results.append(result_output)\n",
    "        answers.append(code_output)\n",
    "        \n",
    "        best_stats[i] = (best, best_count) \n",
    "        question_type_counts[i] = (text_answers, code_answers)\n",
    "        total_outputs[i] = outputs\n",
    "        \n",
    "        total_results[i] = results\n",
    "        total_answers[i] = answers\n",
    "\n",
    "        print(\"code_answers\",code_answers-starting_counts[1],\"text_answers\",text_answers-starting_counts[0])\n",
    "        if DEBUG:\n",
    "            break\n",
    "            \n",
    "    print(f\"Predicted best answer: {best_stats}\")\n",
    "    if VALIDATION:\n",
    "        y.loc[i, 'answer'] = best_stats[i][0]\n",
    "    else:\n",
    "        sample_submission['answer'] = best_stats[i][0]\n",
    "        env.predict(sample_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f26b8e1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:30:29.715166Z",
     "iopub.status.busy": "2024-06-10T15:30:29.714489Z",
     "iopub.status.idle": "2024-06-10T15:30:29.821121Z",
     "shell.execute_reply": "2024-06-10T15:30:29.820164Z"
    },
    "papermill": {
     "duration": 0.122823,
     "end_time": "2024-06-10T15:30:29.823065",
     "exception": false,
     "start_time": "2024-06-10T15:30:29.700242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if VALIDATION:\n",
    "    x['pred'] = y['answer']\n",
    "    print('Correct Answers: {i}'.format(i = sum(x['pred'] == x['answer'])))\n",
    "else:\n",
    "    with open('code.py', 'w') as fout:\n",
    "        fout.write(\"print('done')\")\n",
    "\n",
    "    batcmd = 'timeout 7 ' + sys.executable + ' code.py'\n",
    "    try:\n",
    "        shell_output = subprocess.check_output(batcmd, shell=True).decode('utf8')\n",
    "        print(shell_output)\n",
    "    except:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8365361,
     "sourceId": 73231,
     "sourceType": "competition"
    },
    {
     "datasetId": 4281572,
     "sourceId": 7369493,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4720595,
     "sourceId": 8012825,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4728129,
     "sourceId": 8023365,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4748944,
     "sourceId": 8052555,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 8332,
     "sourceId": 11261,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 8318,
     "sourceId": 11264,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 194.237964,
   "end_time": "2024-06-10T15:30:32.613256",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-10T15:27:18.375292",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "422d1d8c620c4584b8897683a4826f25": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "45f0f40242854000a4a8a050454565eb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "4cd830727ea24211ac46cce854c3c34c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_79fb54c49ecb42c48b56e00fb5d88c76",
        "IPY_MODEL_ef4329cb9d5c4e80a045131f53191908",
        "IPY_MODEL_ffbed51886c245a087e4748ecea76370"
       ],
       "layout": "IPY_MODEL_53e69818ba7648be9030e6c6f9d0b6cf"
      }
     },
     "53e69818ba7648be9030e6c6f9d0b6cf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6ba91e946f764bb5a3bc267d9e081240": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "79fb54c49ecb42c48b56e00fb5d88c76": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_fcd4a493ff59493aa79227bb660ba1ba",
       "placeholder": "​",
       "style": "IPY_MODEL_f12534bdf76a4c1cbd0df9b11f2eea63",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "be4cbfb3e687424398de044a227ab7c4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ef4329cb9d5c4e80a045131f53191908": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_422d1d8c620c4584b8897683a4826f25",
       "max": 3.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_45f0f40242854000a4a8a050454565eb",
       "value": 3.0
      }
     },
     "f12534bdf76a4c1cbd0df9b11f2eea63": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "fcd4a493ff59493aa79227bb660ba1ba": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ffbed51886c245a087e4748ecea76370": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6ba91e946f764bb5a3bc267d9e081240",
       "placeholder": "​",
       "style": "IPY_MODEL_be4cbfb3e687424398de044a227ab7c4",
       "value": " 3/3 [02:25&lt;00:00, 46.86s/it]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
